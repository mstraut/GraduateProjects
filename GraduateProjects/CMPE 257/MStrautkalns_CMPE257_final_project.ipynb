{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Miryam Strautkalns##\n",
        "\n",
        "CMPE 257\n",
        "\n",
        "[Project] Final Results\n"
      ],
      "metadata": {
        "id": "Ovojm-enWutd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Definition:\n",
        "###Data Challenge Objective:\n",
        "* copied from challenge page\n",
        "\n",
        "This yearâ€™s data challenge addresses the problem of fault classification for a rock drill application under different individual configurations of the rock drill. The task is to develop a fault diagnosis/classification model using the provided pressure sensor data as input. The training data consists of data from various faults from five individual configurations, while the testing data for the online leaderboard is blind and is from one individual configuration of the rock drill.  A final validation data set for the final scoring for the competition will be from two individual configurations from the rock drill and the labels will be blind to the contest participants. For both the testing data for the online leaderboard and the final validation data set, a reference condition from a no-fault health condition will also be provided.\n",
        "\n",
        "The training data set contains data from 11 different fault classification categories, in which 10 are different failure modes and one class is from the healthy/no fault condition. The task is to train a model to classify the fault conditions using the training data, and to test this model on the testing data, in which the one submission per day can be used for submitting results to the online leaderboard. Validation is done with a validation data set that will be released for a one-time assessment at the end of the data challenge. Scoring of performance is done through this web interface."
      ],
      "metadata": {
        "id": "cdZJY5agwejM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Project Objectives:\n",
        "\n",
        "The winning group of the 2022 PHM Society Data Challenge achieved an accuracy of 100%. Though to acheive it they used several machine learning techniques and I think that the same or competitive results can be accomplished through a less computationally expensive real world approach. I'm hoping to use an ensemble model to get an accuracy greater than 99.04%."
      ],
      "metadata": {
        "id": "cRGjO0bRrq8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import missingno as msno\n",
        "from pathlib import Path\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, KFold\n"
      ],
      "metadata": {
        "id": "D6yZUkffW9hq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sensor Sampling Description\n",
        "* pin 50kHz Percussion pressure at inlet fitting.\n",
        "* pdmp 50kHz Damper pressure inside the outer chamber.\n",
        "* po 50kHz Pressure in the volume behind the piston.\n",
        "\n",
        "each data point in each data set per sensor is a part of the X set of variables that will relate to the output, the fault value\n",
        "\n",
        "a fault value represented by an int is the **y** value and the **X** value is a cycle of time series data for several sensors. Named **pin**, **pdmp**, and **po**. each y value then corresponds to three np arrays of data as its X."
      ],
      "metadata": {
        "id": "oV8eWUGVqr5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data:"
      ],
      "metadata": {
        "id": "-5_Tsk4ScNO9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Kwfmt4X5WsqM"
      },
      "outputs": [],
      "source": [
        "# methods for creating a dataframe for groups of sensors during a series of tests\n",
        "\n",
        "def get_data_df(data_file):\n",
        "    # creates a DataFrame with organized data for times series fault data\n",
        "    df = pd.read_csv(data_file, header=None, names=range(1000))\n",
        "    df = df.dropna(axis='columns')\n",
        "    df = df.rename({0: 'fault'}, axis='columns')\n",
        "    df = df.loc[:, 'fault':556]\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_df(df_arr):\n",
        "  # combines DataFrames vertically and removes and columns with NaN\n",
        "    full_df = pd.concat(df_arr, axis=1).dropna(axis='columns')\n",
        "    full_df = full_df.loc[:, ~full_df.columns.duplicated()]\n",
        "    return full_df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file paths\n",
        "pin1_file = '/content/data/data_pin1.csv'\n",
        "pdmp1_file = '/content/data/data_pdmp1.csv'\n",
        "po1_file = '/content/data/data_po1.csv'\n",
        "\n",
        "pin2_file = '/content/data/data_pin2.csv'\n",
        "pdmp2_file = '/content/data/data_pdmp2.csv'\n",
        "po2_file = '/content/data/data_po2.csv'\n",
        "\n",
        "pin4_file = '/content/data/data_pin4.csv'\n",
        "pdmp4_file = '/content/data/data_pdmp4.csv'\n",
        "po4_file = '/content/data/data_po4.csv'\n",
        "\n",
        "pin5_file = '/content/data/data_pin5.csv'\n",
        "pdmp5_file = '/content/data/data_pdmp5.csv'\n",
        "po5_file = '/content/data/data_po5.csv'\n",
        "\n",
        "pin6_file = '/content/data/data_pin6.csv'\n",
        "pdmp6_file = '/content/data/data_pdmp6.csv'\n",
        "po6_file = '/content/data/data_po6.csv'\n",
        "\n",
        "# Pin Sensor\n",
        "pin1_data = get_data_df(pin1_file)\n",
        "pin2_data = get_data_df(pin2_file)\n",
        "pin4_data = get_data_df(pin4_file)\n",
        "pin5_data = get_data_df(pin5_file)\n",
        "pin6_data = get_data_df(pin6_file)\n",
        "train_pin = create_df([pin1_data, pin2_data, pin4_data, pin5_data, pin6_data])\n",
        "\n",
        "# PDMP Sensor\n",
        "pdmp1_data = get_data_df(pdmp1_file)\n",
        "pdmp2_data = get_data_df(pdmp2_file)\n",
        "pdmp4_data = get_data_df(pdmp4_file)\n",
        "pdmp5_data = get_data_df(pdmp5_file)\n",
        "pdmp6_data = get_data_df(pdmp6_file)\n",
        "train_pdmp = create_df([pdmp1_data, pdmp2_data, pdmp4_data, pdmp5_data, pdmp6_data])\n",
        "\n",
        "# PO Sensor\n",
        "po1_data = get_data_df(po1_file)\n",
        "po2_data = get_data_df(po2_file)\n",
        "po4_data = get_data_df(po4_file)\n",
        "po5_data = get_data_df(po5_file)\n",
        "po6_data = get_data_df(po6_file)\n",
        "train_po = create_df([po1_data, po2_data, po4_data, po5_data, po6_data])"
      ],
      "metadata": {
        "id": "rkL_qe8RtLLP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Arrays"
      ],
      "metadata": {
        "id": "07jshNr1GU4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data set up for training models\n",
        "#PIN\n",
        "X_pin = train_pin.drop(['fault'], axis=1).values\n",
        "y_pin = train_pin['fault'].values\n",
        "\n",
        "#PDMP\n",
        "X_pdmp = train_pdmp.drop(['fault'], axis=1).values\n",
        "y_pdmp = train_pdmp['fault'].values\n",
        "\n",
        "#PO\n",
        "X_po = train_po.drop(['fault'], axis=1).values\n",
        "y_po = train_po['fault'].values"
      ],
      "metadata": {
        "id": "xJtUZu5owQR6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Validation using K Fold\n",
        "* 5 groups"
      ],
      "metadata": {
        "id": "43_O18zJ5AQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K Fold validation - splitting the data into 5 groups of training and testing. Testing makes up 1/5 of the data.\n",
        "\n",
        "k_fold_dict = {'train': [], 'test': []}\n",
        "kf = KFold(n_splits=5)\n",
        "kf.get_n_splits(X_pin)\n",
        "for i, (train_index, test_index) in enumerate(kf.split(X_pin)):\n",
        "    k_fold_dict['train'].append(train_index)\n",
        "    k_fold_dict['test'].append(test_index)"
      ],
      "metadata": {
        "id": "Yym4ETJxcE_r"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Analysis:\n",
        "\n",
        "I chose to use Random Forest because it struck a solid balance between time training and accuracy. I knew that by using the three models in an ensemble the accuracy would only increase."
      ],
      "metadata": {
        "id": "q1HtSxNVoQHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest\n",
        "\n",
        "Scikit Learn Random Forest [reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
      ],
      "metadata": {
        "id": "MhWsk-j3bn64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rfc(X, y):\n",
        "  # Creates Random Forest Model\n",
        "  clf = RandomForestClassifier()\n",
        "  clf = clf.fit(X, y)\n",
        "  return clf"
      ],
      "metadata": {
        "id": "abwMWu73X3WT"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble model\n",
        "\n",
        "I decided to use the three models as a group to try and increase the accuracy. Similar to the idea of having three people vote and the majority vote decide the class choice."
      ],
      "metadata": {
        "id": "GbAULKuSdoCE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 1:\n",
        "* In the first ensemble model I let each model predict the fault based on it's individual choice and let the majority select the fault for prediction.\n",
        "\n",
        "##Version 1 Problem:\n",
        "\n",
        "There are instances where fault data appears similar for two sensors, but is more distinct for the third, showing that the physical system is affected differently depending on the fault. One Fault will be less reliably predicted on a model of a sensor less impacted. This caused false predicts.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Predictions:**\n",
        "* 0.9981203007518797\n",
        "* 0.9981203007518797\n",
        "* 0.9987460815047022\n",
        "* 0.9968652037617555\n",
        "* 0.9974921630094044\n",
        "\n",
        "Average Accuracy: 0.9979"
      ],
      "metadata": {
        "id": "WmDZ-ZV5XRok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ensemble_predict(train_i, test_i):\n",
        "  # Variables for model\n",
        "  clf_pin = rfc(X_pin[train_i], y_pin[train_i])\n",
        "  clf_pdmp = rfc(X_pdmp[train_i], y_pdmp[train_i])\n",
        "  clf_po = rfc(X_po[train_i], y_po[train_i])\n",
        "  predict_pin = rfc(X_pin[train_i], y_pin[train_i]).predict(X_pin[test_i])\n",
        "  predict_pdmp = rfc(X_pdmp[train_i], y_pdmp[train_i]).predict(X_pdmp[test_i])\n",
        "  predict_po = rfc(X_po[train_i], y_po[train_i]).predict(X_po[test_i])\n",
        "  y_test = y_pin[test_i]\n",
        "  index_int = 0\n",
        "  error_int = 0\n",
        "  total_test = len(test_i)\n",
        "\n",
        "  # loop for checking accuracy\n",
        "  for pin, pdmp, po, y in zip(predict_pin, predict_pdmp, predict_po, y_test):\n",
        "    predict = np.bincount([pin, pdmp, po]).argmax()\n",
        "    # if the prediction doesn't match the expected value\n",
        "    if predict != y:\n",
        "      # shows the data for the failed prediction\n",
        "      error_int = error_int + 1\n",
        "\n",
        "      # - ERROR TRACKING -\n",
        "      # print('PIN prediction:', pin, '| PDMP prediction:', pdmp, '| PO prediction:', po, '| Expected Fault:', y)\n",
        "      # print('Fault Probabilities: ')\n",
        "      # print(clf_pin.predict_proba([X_pin[test_i][index_int]]))\n",
        "      # print(clf_pdmp.predict_proba([X_pdmp[test_i][index_int]]))\n",
        "      # print(clf_po.predict_proba([X_po[test_i][index_int]]))\n",
        "\n",
        "    index_int = index_int + 1\n",
        "  print('accuracy: ', (total_test - error_int)/total_test)"
      ],
      "metadata": {
        "id": "qIeSM1hfdUia"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_num = 1\n",
        "for group in zip(k_fold_dict['train'], k_fold_dict['test']):\n",
        "  print('\\nValidation Group', group_num)\n",
        "  ensemble_predict(group[0], group[1])\n",
        "  group_num = group_num + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip6Rhl72PkUk",
        "outputId": "9003796f-f820-49af-e78d-c58981b3b5c4"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Group 1\n",
            "accuracy:  0.9993734335839599\n",
            "\n",
            "Validation Group 2\n",
            "accuracy:  0.9981203007518797\n",
            "\n",
            "Validation Group 3\n",
            "accuracy:  0.9981191222570532\n",
            "\n",
            "Validation Group 4\n",
            "accuracy:  0.9981191222570532\n",
            "\n",
            "Validation Group 5\n",
            "accuracy:  0.9981191222570532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion on Version 1:\n",
        "\n",
        "I thought that I could acheive a greater accuracy by assessing the instances where the predictions were wrong. I decided to proceed with a 'Version 2', which switched to a system assessing probability because I observed that they were mostly situations where one model sensor was more reliable than the others, but this was over-ruled by the predictions of two sensors with low probability in choosing their prediction."
      ],
      "metadata": {
        "id": "nvH7xRDktXk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Version 2:\n",
        "* In this version rather than allowing each predict method to decide the fault similar to a majority vote between 3 people, I summed the three arrays of probabilities for each sensor model and chose the fault that had the highest sum of probabilities. This increased the accuracy to 100% during the preliminary tests. I've listed the accuracy from cross validation tests.\n",
        "\n",
        "## Evaluation and Reflection:\n",
        "\n",
        "* Occasionally there is one fault that is classified incorrectly. This fault is a unique scenario that cannot be solved through machine learning. The probability is equally split between two faults which means this one scenario requires domain knowledge. This still isn't something I consider a failing, the models in ensemble have been able to successfully classify every no fault and fault with 100% accuracy.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Predictions:**\n",
        "* 1.0\n",
        "* 1.0\n",
        "* 0.9993730407523511\n",
        "* 0.9987460815047022\n",
        "* 1.0\n",
        "\n",
        "Average Accuracy: 0.9996"
      ],
      "metadata": {
        "id": "0MgRXc0kX_0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ensemble_predict_v2(train_i, test_i):\n",
        "  # Variables for model\n",
        "  clf_pin = rfc(X_pin[train_i], y_pin[train_i])\n",
        "  clf_pdmp = rfc(X_pdmp[train_i], y_pdmp[train_i])\n",
        "  clf_po = rfc(X_po[train_i], y_po[train_i])\n",
        "  predict_pin = clf_pin.predict(X_pin[test_i])\n",
        "  predict_pdmp = clf_pdmp.predict(X_pdmp[test_i])\n",
        "  predict_po = clf_po.predict(X_po[test_i])\n",
        "  pin_predict_prob = clf_pin.predict_proba(X_pin[test_i])\n",
        "  pdmp_predict_prob = clf_pdmp.predict_proba(X_pdmp[test_i])\n",
        "  po_predict_prob = clf_po.predict_proba(X_po[test_i])\n",
        "  predict_prob = pin_predict_prob + pdmp_predict_prob + po_predict_prob\n",
        "  y_test = y_pin[test_i]\n",
        "  index_int = 0\n",
        "  error_int = 0\n",
        "  total_test = len(test_i)\n",
        "\n",
        "  # loop for checking accuracy\n",
        "  for prob_x, pin, pdmp, po, y in zip(predict_prob, predict_pin, predict_pdmp, predict_po, y_test):\n",
        "    fault = np.where(prob_x == prob_x.max())[0][0]+1\n",
        "    # if the prediction doesn't match the expected value\n",
        "    if fault != y:\n",
        "      # shows the data for the failed prediction\n",
        "      error_int = error_int + 1\n",
        "\n",
        "      # - ERROR TRACKING -\n",
        "      # print('Probability Prediction:', fault)\n",
        "      # print('PIN prediction:', pin, '| PDMP prediction:', pdmp, '| PO prediction:', po, '| Expected Fault:', y)\n",
        "      # print('Fault Probabilities: ')\n",
        "      # print(clf_pin.predict_proba([X_pin[test_i][index_int]]))\n",
        "      # print(clf_pdmp.predict_proba([X_pdmp[test_i][index_int]]))\n",
        "      # print(clf_po.predict_proba([X_po[test_i][index_int]]))\n",
        "\n",
        "    index_int = index_int + 1\n",
        "  print('accuracy: ', (total_test - error_int)/total_test)"
      ],
      "metadata": {
        "id": "BWBPv0yOVy23"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_num = 1\n",
        "for group in zip(k_fold_dict['train'], k_fold_dict['test']):\n",
        "  print('\\nValidation Group', group_num)\n",
        "  ensemble_predict_v2(group[0], group[1])\n",
        "  group_num = group_num + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIqAyISkcFRb",
        "outputId": "5776f9c8-cf66-49b5-d2ed-f44e71c697e3"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Group 1\n",
            "accuracy:  1.0\n",
            "\n",
            "Validation Group 2\n",
            "accuracy:  1.0\n",
            "\n",
            "Validation Group 3\n",
            "accuracy:  0.9993730407523511\n",
            "\n",
            "Validation Group 4\n",
            "accuracy:  0.9987460815047022\n",
            "\n",
            "Validation Group 5\n",
            "accuracy:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Results:**\n",
        "\n",
        "[2022 PHM Society Data Challenge](https://data.phmsociety.org/2022-phm-conference-data-challenge/)\n",
        "\n",
        "Accuracy of winners:\n",
        "\n",
        "1. **100.00%**\n",
        "2. **99.77%**\n",
        "3. **99.04%**\n",
        "\n",
        "My goal was to score at least high enough to place in the top three for this data challenge competition. Based on my own testing I can confidently claim 2nd place with an average accuracy of **99.96%** with 5 cross validation tests and with a solution that can be used in the real world. Thus, this project was successful. I have submitted my validation results for their mystery validation data set and will get the results back soon.\n",
        "\n"
      ],
      "metadata": {
        "id": "k-HtXDeAkorn"
      }
    }
  ]
}